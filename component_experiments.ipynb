{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a60cacc-a5e2-4c33-a3c2-25ce9398ad5a",
   "metadata": {},
   "source": [
    "# Measuring LLM Performance\n",
    "\n",
    "In this notebook, we use the bespoke tooling to perform experiments to understand how to build a METCLOUD chatbot.\n",
    "Firstly, we define a METCLOUD test set; this consists of 119 questions and answer pairs that we use to test Language Models deployed locally on the machine. We then set up a series of experiments to:\n",
    "1. Ask a given LLM the questions via 'ollama'\n",
    "2. Collect the responses\n",
    "3. Compare/judge the comparison between the LLM response and the ground-truth\n",
    "\n",
    "These experiments are varied as follows:\n",
    "1. We do this for different open-source language models: Phi-3, Llama 3, Llama 3.1, Qwen-2, Mistral 7B and Gemma 2\n",
    "2. We include no RAG, Naive RAG and Advanced RAG for each language model\n",
    "3. For Naive RAG and Advanced RAG, we use different retrieval datasets AND different embedding models\n",
    "4. We repeat the experiments using six _fine-tuned_ versions of the above models\n",
    "\n",
    "This allows us to collect comprehensive performance metrics for each setup to determine the best core Chatbot pipeline!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9416ab-ec93-482b-936b-e1e56e774ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08cd911-f0f9-46a5-be7c-99b2542f391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "from pathlib import Path\n",
    "\n",
    "from src.qa import QuestionAnswering\n",
    "from src.vectordb import ChromaDB\n",
    "from src.verifier import Verifier\n",
    "from src.reporting import folder_to_dataframe #collate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d9cce-7056-4682-b4f9-4c6ff91bcfe7",
   "metadata": {},
   "source": [
    "# Custom Tools\n",
    "\n",
    "### `QuestionAnswering`\n",
    "This class takes a pandas dataframe containing our question/answer pairs in our 119 test set and decomposes them into records. It will then loop through each record, and use `ollama-python` to ask a LLM the question, before collecting its response and storing it in a new dataframe alongside the original question and answer.\n",
    "- System prompt is as follows:\n",
    "  ```You are to be a human-like, compassionate, friendly and polite\n",
    "                You are to be a human-like, compassionate, friendly and polite\n",
    "                chatbot assistant for a cyber security firm. You will be asked customer support\n",
    "                questions and it is your job to answer those questions. You aim to answer all\n",
    "                queries, and if you are unsure you will ask the customer to hold while they\n",
    "                are transferred to a human agent.\n",
    "- The user prompt is the question!\n",
    "- This tool can be passed a `ChromaDB` object to enable RAG.\n",
    "- By default, this will add the answer from the closest matching question as context to the user prompt\n",
    "- Advanced adopts re-ranking and the top three question answers\n",
    "\n",
    "### `ChromaDB`\n",
    "ChromaDB is a class to wraps around a `Chroma` object. This can be passed a DataFrame and the name of a `HuggingFace` embedding function, and it will vector embed our Q&A pairs in the dataframes as documents, where the vector is the `question` and the answer is stored as metadata.\n",
    "- Implements a `retrieve` function which will take a prompt and return the metadata of the closest matching prompt using the embedding model\n",
    "- This will add the top 1 matching questions' answer to the `user-prompt` for NAIVE mode\n",
    "- Implements a re-ranker approach if requested using a CrossEncoder i.e adding top 3 _reranked_ questions answers to the user prompt for ADVANCED\n",
    "\n",
    "### `Verifier`\n",
    "The verifier is another wrapper around Ollama. Its sole purpose is to take two pieces of information and compare them for similarity and agreement. This uses chain-of-thought prompting and justification-forcing to improve the performance. We use this to take each answer and generated-answer and compare them for consistency! This uses an underlying language model to do this;\n",
    "- System prompt is now much more complicated; CoT comes from the request for justification; a novel way to do CoT!\n",
    "  ```# YOUR ROLE\n",
    "    You are a question and answering validation capability. You can accurately compare two potential pieces of text for similarity and consistency.\n",
    "    \n",
    "    # YOUR TASK\n",
    "    Your task is to assess / judge if information A : <gen_response>\n",
    "    IS CONSISTENT with information B: <ground_truth>\n",
    "    Information B should be treated as the TRUTH even if you disagree with its content. If the text samples contain similar and non-conflicting information, then then you should judge them as consistent. \n",
    "    \n",
    "    # OUTPUT INSTRUCTIONS\n",
    "    Return your judgement as a JSON compatible dictionary. An example of this format is:\n",
    "    \n",
    "    {\"consistent\": \"(either \\\"True\\\" or \\\"False\\\")\", \"justification\": \"(description why the samples are consistent)\"}\n",
    "    \n",
    "    Your output should only contain the \"consistent\" and \"justification\". Do not act as an assistant and do not yap. Make sure your output is valid JSON.```\n",
    "- User prompt is as simple as `please compare this information`!\n",
    "- We use regex to guarantee that the LLM has returned information in the format we expect.\n",
    "\n",
    "We do this twice using different LLM's and take the average accuracy to test its efficacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eeb0c6-ea43-4cd1-98df-eb8d142c1031",
   "metadata": {},
   "source": [
    "# Running the Experiments\n",
    "\n",
    "We have six off-the-shelf LLMs and six finetuned LLMs (trained using the full 2500 metcloud dataset (via Unsloth))\n",
    "\n",
    "We want to understand:\n",
    "- The raw performance (i.e No RAG)\n",
    "- Performance with Naive RAG and Advanced RAG, comparing:\n",
    "  - Two different embedding functions for RAG (MiniLM, mpnet)\n",
    "  - Using test dataset as lookup; using remainder dataset as lookup\n",
    "\n",
    "Therefore, for each LLM, we have NINE sets of results. (i.e 8 RAG, one Raw)\n",
    "We use the code below to define an experiment\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6e86ad-f168-4e40-b24b-dbe229219f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: test\n",
      "embedding_model: all-MiniLM-L6-v2\n",
      "rag: True\n",
      "advanced: False\n",
      "chromadb: <src.vectordb.ChromaDB object at 0x7f090d286750>\n",
      "save_dir: /data/Demonstration/dataset_test_emb_all-MiniLM-L6-v2_rerank_False\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "#this is the path i.e. location to where our datasets are stored on our PC\n",
    "data_path = Path('/data/')\n",
    "core_dir  = data_path/'Demonstration' #folder where we save everything during experiment\n",
    "\n",
    "#now we set our 'experiment' parameters\n",
    "rag       = True                            #this means that we do use RAG\n",
    "mode      = 'test'                          #this means we use the test set i.e. 119 questions for rag\n",
    "advanced  = False                           #this means we use RERANKING when doing RAG i.e ADVANCED otherwise NAIVE.\n",
    "emb_model = \"all-MiniLM-L6-v2\"              #this is the RAG embedding model\n",
    "#emb_model = \"all-mpnet-base-v2\"\n",
    "\n",
    "if rag == True:\n",
    "    #load data to store in vector database for RAG - context\n",
    "    if mode == \"test\":\n",
    "        rag_data = pd.read_csv(data_path/'metcloud-with-id.csv')  #put test set into vector database (119)\n",
    "    else:\n",
    "        rag_data = pd.read_csv(data_path/'METCLOUD_training.csv') #removed the 119 questions in large dataset (2500 - 119) for rag never used full 2500\n",
    "\n",
    "    #we now create a vector database of Q's using our ChromaDB class that we wrote\n",
    "    #firstly, we create a cache folder to store our embedding model\n",
    "    #then we pass our dataset and chroma will automatically embed the questions\n",
    "    chroma_cache = data_path/f'chroma_cache/chromadb_{mode}_{emb_model}'\n",
    "    chroma_db    = ChromaDB(chroma_cache,\n",
    "                            rag_data,\n",
    "                            embedding_model = emb_model)\n",
    "    \n",
    "    #create directories to store our generated answers\n",
    "    save_dir = core_dir/f'dataset_{mode}_emb_{emb_model}_rerank_{advanced}'\n",
    "    save_dir.mkdir(exist_ok=True,parents=True) #make_directory\n",
    "else:\n",
    "    #we do not use RAG!\n",
    "    chroma_db = None\n",
    "    save_dir = core_dir/'no_rag'\n",
    "    save_dir.mkdir(exist_ok=True,parents=True) #make_directory\n",
    "\n",
    "#119 questions to run through pipeline (always test set)\n",
    "data_df =  pd.read_csv(data_path/'metcloud-with-id.csv')\n",
    "\n",
    "#print out information\n",
    "print('mode:',mode)\n",
    "print('embedding_model:',emb_model)\n",
    "print('rag:',rag)\n",
    "print('advanced:',advanced)\n",
    "print('chromadb:',chroma_db)\n",
    "print('save_dir:',save_dir)\n",
    "\n",
    "#three questions for demonstration\n",
    "data_df = data_df.head(3)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd9c226-1228-4b7a-be98-25797f3681d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is METCLOUD?</td>\n",
       "      <td>METCLOUD is a multi-award-winning secure sover...</td>\n",
       "      <td>METCLOUD is a secure sovereign cloud service p...</td>\n",
       "      <td>69a9382c7a9840248efc5d8851750530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to contact METCLOUD?</td>\n",
       "      <td>If you have a question about how to easily ado...</td>\n",
       "      <td>To contact METCLOUD, you can reach out to them...</td>\n",
       "      <td>327820806ca0495ca8360551897782c6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are essential reasons to choose METCLOUD ...</td>\n",
       "      <td>METCLOUD 'Get Connected Cyber Safe' is our tra...</td>\n",
       "      <td>Choosing METCLOUD powered by HPE GreenLake for...</td>\n",
       "      <td>6d1765b7c3cc430196699c2b4e6c7e19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                  What is METCLOUD?   \n",
       "1                           How to contact METCLOUD?   \n",
       "2  What are essential reasons to choose METCLOUD ...   \n",
       "\n",
       "                                             context  \\\n",
       "0  METCLOUD is a multi-award-winning secure sover...   \n",
       "1  If you have a question about how to easily ado...   \n",
       "2  METCLOUD 'Get Connected Cyber Safe' is our tra...   \n",
       "\n",
       "                                            response  \\\n",
       "0  METCLOUD is a secure sovereign cloud service p...   \n",
       "1  To contact METCLOUD, you can reach out to them...   \n",
       "2  Choosing METCLOUD powered by HPE GreenLake for...   \n",
       "\n",
       "                                 id  \n",
       "0  69a9382c7a9840248efc5d8851750530  \n",
       "1  327820806ca0495ca8360551897782c6  \n",
       "2  6d1765b7c3cc430196699c2b4e6c7e19  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6655e3-4f94-47c2-a737-1eef62dcec7e",
   "metadata": {},
   "source": [
    "Quickly check that the chromadb is working as expected. Ask a question, see if we get an appropriate qa pair back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6b3b3b-cadf-444e-8a24-f0cfff822c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"METCLOUD is a secure sovereign cloud service provider that specializes in offering digital modernization through advanced cybersecurity and artificial intelligence. It is designed to support businesses in adopting next-generation technologies for cloud computing and cybersecurity, ensuring that they stay secure, effective, and efficient. METCLOUD's approach is tailored to meet the unique needs of businesses, with a focus on a people-first strategy. The platform is scalable, making it suitable for small to medium-sized enterprises, and it has been recognized for its excellence in the field, including being named the Cybersecurity Firm of the Year by Finance Monthly in the 2021 FinTech Awards.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_db.retrieve('what is metcloud',k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c843f-2995-4a52-aa8e-151080824586",
   "metadata": {},
   "source": [
    "### Now we run the Question and Answering loop!\n",
    "In this cell, we do the following:\n",
    "1. Define a list of open source models, available on Ollama.\n",
    "2. Write a for loop to go through each model.\n",
    "3. 'pull' the model -> this downloads it, if we don't already have it\n",
    "4. Creates a `QuestionAnswering` class.\n",
    "5. Processes the data\n",
    "6. Asks each question in the dataset and stores the results to our 'save_dir' set earlier. We can pass in our Chromadb to enable RAG. This will be either niave or advanced, depending on the 'advanced' flag set earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b2e81b-e25f-4a5e-b83b-d19830e210f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'metcloud_1epoch_Qwen2-7B-instruct-bnb-4bit:latest',\n",
       "   'model': 'metcloud_1epoch_Qwen2-7B-instruct-bnb-4bit:latest',\n",
       "   'modified_at': '2024-09-01T20:32:24.9618355Z',\n",
       "   'size': 4683072814,\n",
       "   'digest': '68c62ba5086d21af76fbd5687a392602e7231f9bc77190f9ad3ba442c99ab697',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'qwen2',\n",
       "    'families': ['qwen2'],\n",
       "    'parameter_size': '7.6B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'metcloud_1epoch_Phi-3-mini-4k-instruct-bnb-4bit:latest',\n",
       "   'model': 'metcloud_1epoch_Phi-3-mini-4k-instruct-bnb-4bit:latest',\n",
       "   'modified_at': '2024-09-01T20:31:35.465515Z',\n",
       "   'size': 2318921171,\n",
       "   'digest': '05154fcab5df76c54623459893db627ac9addf51c8cb538177dbfc49429fc870',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '3.8B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'metcloud_1epoch_mistral-7b-instruct-v0.3-bnb-4bit:latest',\n",
       "   'model': 'metcloud_1epoch_mistral-7b-instruct-v0.3-bnb-4bit:latest',\n",
       "   'modified_at': '2024-09-01T20:31:11.8877564Z',\n",
       "   'size': 4372813691,\n",
       "   'digest': 'bd03864078d27df57ccc93755ec81c871d98047fc7876e4fff5c11d54befe8fb',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '7.2B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'metcloud_1epoch_Meta-Llama-3.1-8B-Instruct-bnb-4bit:latest',\n",
       "   'model': 'metcloud_1epoch_Meta-Llama-3.1-8B-Instruct-bnb-4bit:latest',\n",
       "   'modified_at': '2024-09-01T20:30:25.3379023Z',\n",
       "   'size': 4920736147,\n",
       "   'digest': 'da93055867b304f95f1a4b506ba27b3ef7251fa94b14ce412bc2a6b51492a4be',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'metcloud_1epoch_llama-3-8b-Instruct-bnb-4bit:latest',\n",
       "   'model': 'metcloud_1epoch_llama-3-8b-Instruct-bnb-4bit:latest',\n",
       "   'modified_at': '2024-09-01T20:29:33.8239101Z',\n",
       "   'size': 4920735771,\n",
       "   'digest': '73290aefb616105a40efe08421cf63bf609b159723a74213492b493a98cd400c',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'metcloud_1epoch_gemma-2-9b-it-bnb-4bit:latest',\n",
       "   'model': 'metcloud_1epoch_gemma-2-9b-it-bnb-4bit:latest',\n",
       "   'modified_at': '2024-09-01T20:28:42.5898703Z',\n",
       "   'size': 5761058980,\n",
       "   'digest': '7b859e0342b2a58de454c56b1a429703526eac23f429196b65b467420c61b481',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'gemma2',\n",
       "    'families': ['gemma2'],\n",
       "    'parameter_size': '9.2B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'gemma2:latest',\n",
       "   'model': 'gemma2:latest',\n",
       "   'modified_at': '2024-08-30T08:34:36.7496947Z',\n",
       "   'size': 5443152417,\n",
       "   'digest': 'ff02c3702f322b9e075e9568332d96c0a7028002f1a5a056e0a6784320a4db0b',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'gemma2',\n",
       "    'families': ['gemma2'],\n",
       "    'parameter_size': '9.2B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3.1:latest',\n",
       "   'model': 'llama3.1:latest',\n",
       "   'modified_at': '2024-08-30T08:08:00.5149823Z',\n",
       "   'size': 4661230720,\n",
       "   'digest': 'f66fc8dc39ea206e03ff6764fcc696b1b4dfb693f0b6ef751731dd4e6269046e',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'qwen2:latest',\n",
       "   'model': 'qwen2:latest',\n",
       "   'modified_at': '2024-08-28T09:56:39.2024866Z',\n",
       "   'size': 4431400262,\n",
       "   'digest': 'e0d4e1163c585612b5419eee0a0d87fe0cac5fa6844001392e78d0cdf57c8138',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'qwen2',\n",
       "    'families': ['qwen2'],\n",
       "    'parameter_size': '7.6B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3:latest',\n",
       "   'model': 'llama3:latest',\n",
       "   'modified_at': '2024-08-28T09:29:23.1599368Z',\n",
       "   'size': 4661224676,\n",
       "   'digest': '365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'mistral:latest',\n",
       "   'model': 'mistral:latest',\n",
       "   'modified_at': '2024-08-28T08:53:37.0511413Z',\n",
       "   'size': 4113301824,\n",
       "   'digest': 'f974a74358d62a017b37c6f424fcdf2744ca02926c4f952513ddf474b2fa5091',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '7.2B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'phi3:latest',\n",
       "   'model': 'phi3:latest',\n",
       "   'modified_at': '2024-08-28T08:37:44.9980013Z',\n",
       "   'size': 2176178913,\n",
       "   'digest': '4f222292793889a9a40a020799cfd28d53f3e01af25d48e06c5e708610fc47e9',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'phi3',\n",
       "    'families': ['phi3'],\n",
       "    'parameter_size': '3.8B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc124d5e-79a0-4f8b-aa27-37161764cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: llama3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:57<00:00, 59.13s/it] \n"
     ]
    }
   ],
   "source": [
    "#off the shelf models\n",
    "#models = ['phi3','mistral','gemma2','llama3','llama3.1','qwen2']\n",
    "\n",
    "#demonstration, commented out other models\n",
    "\n",
    "#finetuned models\n",
    "#models = [i['name'] for i in ollama.list()['models'] if 'metcloud' in i['name']]\n",
    "\n",
    "models = ['llama3.1']\n",
    "\n",
    "#loop through each of the models in models\n",
    "for model in models:\n",
    "  #download the model if we dont have it  \n",
    "  #ollama.pull(model)\n",
    "  print('MODEL:',model)\n",
    "\n",
    "  #create question/answering class, sourced at top of notebook\n",
    "  qa = QuestionAnswering(model=model)\n",
    "\n",
    "  #process the dataset into a list\n",
    "  qa.process_dataset(data_df)\n",
    "\n",
    "  #ask all questions, saving responses to a .csv file in save_dir\n",
    "  qa.ask_all_questions(save_dir,\n",
    "                       vector_db=chroma_db,\n",
    "                       advanced = advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7d8af-d536-4a86-9fb6-ecf1782a2267",
   "metadata": {},
   "source": [
    "### Now we do verification i.e. how good were the LLM responses?\n",
    "\n",
    "1. We grab our answers generated in the previous cell\n",
    "2. We define a set of 'verification' models. In this case, llama3.1 and gemma2\n",
    "3. Loop through these models, create a verifier\n",
    "4. Loop through the generated answers and check if the generated answers were good using verifier\n",
    "5. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ad99ba-70ae-4a93-96cb-e879b9d9d6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.69s/it]\n",
      "100%|██████████| 3/3 [00:42<00:00, 14.15s/it]\n"
     ]
    }
   ],
   "source": [
    "#this runs after top half has completed all variations of experiments\n",
    "#We first pick which set to verify\n",
    "#advanced = True #False (naive)\n",
    "#mode   = 'test'\n",
    "#emb_model = \"all-MiniLM-L6-v2\" #fastest\n",
    "#data_path = Path('/data/')\n",
    "\n",
    "#this is the list of tested models\n",
    "#models = ['gemma2','phi3','qwen2','llama3','llama3.1','mistral']\n",
    "\n",
    "#now we define our verifier models i.e vmodels\n",
    "vmodels = ['llama3.1','gemma2']\n",
    "\n",
    "#we loop through our verifier models. This is just two loops\n",
    "for vmodel in vmodels:\n",
    "  #makes sure we definitely have the verifier model  \n",
    "  #ollama.pull(vmodel)\n",
    "\n",
    "  #now we create a verifier class, using our verifier model\n",
    "  verifier = Verifier(vmodel)\n",
    "\n",
    "  #'this' is just a directory where we will store our results\n",
    "  #this  = f'dataset_{mode}_emb_{emb_model}_rerank_{advanced}'\n",
    "  this  = data_path/f'Verification/{this}'\n",
    "  this.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "  #loop through model folders in the question answering directory \n",
    "  for model in models:\n",
    "\n",
    "    #create a directory in our verifier directory to store these results\n",
    "    pth = this/f'{vmodel}'\n",
    "    pth.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "    #load the csv containing all of the results\n",
    "    answer_df = pd.read_csv(f'{save_dir}/{model}/all_questions.csv')\n",
    "\n",
    "    #get verifier to see how good the responses were\n",
    "    verifier.judge_all_questions(answer_df,model,pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d153de9-6163-4fac-a015-29ddf8626963",
   "metadata": {},
   "source": [
    "### Reporting the Performance\n",
    "\n",
    "Now we have built a method that looks into our verification folder and pulls out all of the performance metrics for each verifier run per model. We get the time, tokens per second and the accuracy averaged across the two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f8bb74-137d-4b19-87ed-50877e443190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time</th>\n",
       "      <th>tps</th>\n",
       "      <th>gemma2_accuracy</th>\n",
       "      <th>llama3.1_accuracy</th>\n",
       "      <th>average_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.1</td>\n",
       "      <td>59.099569</td>\n",
       "      <td>25.984397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model       time        tps  gemma2_accuracy  llama3.1_accuracy  \\\n",
       "0  llama3.1  59.099569  25.984397              1.0                1.0   \n",
       "\n",
       "   average_accuracy  \n",
       "0               1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_to_dataframe(this,model_list=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e67e4018-7897-4754-9593-fe4a9edb4b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time</th>\n",
       "      <th>tps</th>\n",
       "      <th>gemma2_accuracy</th>\n",
       "      <th>llama3.1_accuracy</th>\n",
       "      <th>average_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2</td>\n",
       "      <td>7.174388</td>\n",
       "      <td>22.264301</td>\n",
       "      <td>0.747899</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.710084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3.1</td>\n",
       "      <td>6.479725</td>\n",
       "      <td>32.016442</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.957983</td>\n",
       "      <td>0.924370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3</td>\n",
       "      <td>7.445677</td>\n",
       "      <td>31.509900</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.831933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral</td>\n",
       "      <td>6.168018</td>\n",
       "      <td>33.226657</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.873950</td>\n",
       "      <td>0.865546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phi3</td>\n",
       "      <td>7.622237</td>\n",
       "      <td>51.759784</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.949580</td>\n",
       "      <td>0.899160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen2</td>\n",
       "      <td>7.666361</td>\n",
       "      <td>33.297824</td>\n",
       "      <td>0.907563</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model      time        tps  gemma2_accuracy  llama3.1_accuracy  \\\n",
       "0    gemma2  7.174388  22.264301         0.747899           0.672269   \n",
       "1  llama3.1  6.479725  32.016442         0.890756           0.957983   \n",
       "2    llama3  7.445677  31.509900         0.840336           0.823529   \n",
       "3   mistral  6.168018  33.226657         0.857143           0.873950   \n",
       "4      phi3  7.622237  51.759784         0.848739           0.949580   \n",
       "5     qwen2  7.666361  33.297824         0.907563           0.974790   \n",
       "\n",
       "   average_accuracy  \n",
       "0          0.710084  \n",
       "1          0.924370  \n",
       "2          0.831933  \n",
       "3          0.865546  \n",
       "4          0.899160  \n",
       "5          0.941176  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_to_dataframe('/data/Verification/no_rag/',model_list=['phi3','llama3.1','gemma2','qwen2','llama3','mistral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c37582e-da5f-4a73-b0c1-ac54593ac263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n",
      "Skipping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [06:02<00:00,  3.04s/it] \n",
      "100%|██████████| 119/119 [03:52<00:00,  1.96s/it]\n",
      "100%|██████████| 119/119 [04:14<00:00,  2.14s/it]\n",
      "100%|██████████| 119/119 [04:48<00:00,  2.43s/it]\n",
      "100%|██████████| 119/119 [04:42<00:00,  2.37s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [07:54<00:00,  3.99s/it]\n",
      "100%|██████████| 119/119 [07:54<00:00,  3.99s/it]\n",
      "100%|██████████| 119/119 [08:01<00:00,  4.04s/it]\n",
      "100%|██████████| 119/119 [04:30<00:00,  2.27s/it]\n",
      "100%|██████████| 119/119 [07:59<00:00,  4.03s/it]\n",
      "100%|██████████| 119/119 [08:16<00:00,  4.17s/it]\n",
      "100%|██████████| 119/119 [07:59<00:00,  4.03s/it]\n",
      "100%|██████████| 119/119 [07:30<00:00,  3.78s/it]\n",
      "100%|██████████| 119/119 [08:05<00:00,  4.08s/it]\n",
      "100%|██████████| 119/119 [08:12<00:00,  4.14s/it]\n",
      "100%|██████████| 119/119 [08:33<00:00,  4.32s/it]\n",
      "100%|██████████| 119/119 [08:36<00:00,  4.34s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [05:59<00:00,  3.02s/it] \n",
      "100%|██████████| 119/119 [04:18<00:00,  2.17s/it]\n",
      "100%|██████████| 119/119 [04:21<00:00,  2.20s/it]\n",
      "100%|██████████| 119/119 [02:28<00:00,  1.25s/it]\n",
      "100%|██████████| 119/119 [04:05<00:00,  2.07s/it]\n",
      "100%|██████████| 119/119 [04:08<00:00,  2.09s/it]\n",
      "100%|██████████| 119/119 [03:59<00:00,  2.02s/it]\n",
      "100%|██████████| 119/119 [04:14<00:00,  2.14s/it]\n",
      "100%|██████████| 119/119 [03:50<00:00,  1.94s/it]\n",
      "100%|██████████| 119/119 [04:28<00:00,  2.25s/it]\n",
      "100%|██████████| 119/119 [04:46<00:00,  2.41s/it]\n",
      "100%|██████████| 119/119 [04:43<00:00,  2.39s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [08:10<00:00,  4.12s/it]\n",
      "100%|██████████| 119/119 [07:42<00:00,  3.89s/it]\n",
      "100%|██████████| 119/119 [07:46<00:00,  3.92s/it]\n",
      "100%|██████████| 119/119 [04:21<00:00,  2.20s/it]\n",
      "100%|██████████| 119/119 [07:54<00:00,  3.99s/it]\n",
      "100%|██████████| 119/119 [07:49<00:00,  3.95s/it]\n",
      "100%|██████████| 119/119 [07:34<00:00,  3.82s/it]\n",
      "100%|██████████| 119/119 [07:23<00:00,  3.73s/it]\n",
      "100%|██████████| 119/119 [07:44<00:00,  3.91s/it]\n",
      "100%|██████████| 119/119 [07:55<00:00,  4.00s/it]\n",
      "100%|██████████| 119/119 [08:21<00:00,  4.21s/it]\n",
      "100%|██████████| 119/119 [08:30<00:00,  4.29s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [06:00<00:00,  3.03s/it] \n",
      "100%|██████████| 119/119 [04:07<00:00,  2.08s/it]\n",
      "100%|██████████| 119/119 [04:10<00:00,  2.11s/it]\n",
      "100%|██████████| 119/119 [02:29<00:00,  1.25s/it]\n",
      "100%|██████████| 119/119 [04:02<00:00,  2.03s/it]\n",
      "100%|██████████| 119/119 [04:05<00:00,  2.06s/it]\n",
      "100%|██████████| 119/119 [04:02<00:00,  2.04s/it]\n",
      "100%|██████████| 119/119 [04:12<00:00,  2.12s/it]\n",
      "100%|██████████| 119/119 [03:46<00:00,  1.90s/it]\n",
      "100%|██████████| 119/119 [04:08<00:00,  2.09s/it]\n",
      "100%|██████████| 119/119 [04:44<00:00,  2.39s/it]\n",
      "100%|██████████| 119/119 [04:40<00:00,  2.35s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [07:56<00:00,  4.01s/it]\n",
      "100%|██████████| 119/119 [07:45<00:00,  3.91s/it]\n",
      "100%|██████████| 119/119 [07:44<00:00,  3.90s/it]\n",
      "100%|██████████| 119/119 [04:22<00:00,  2.20s/it]\n",
      "100%|██████████| 119/119 [07:55<00:00,  4.00s/it]\n",
      "100%|██████████| 119/119 [07:49<00:00,  3.94s/it]\n",
      "100%|██████████| 119/119 [07:49<00:00,  3.94s/it]\n",
      "100%|██████████| 119/119 [07:10<00:00,  3.62s/it]\n",
      "100%|██████████| 119/119 [08:02<00:00,  4.05s/it]\n",
      "100%|██████████| 119/119 [07:58<00:00,  4.02s/it]\n",
      "100%|██████████| 119/119 [08:19<00:00,  4.20s/it]\n",
      "100%|██████████| 119/119 [08:06<00:00,  4.09s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [06:00<00:00,  3.03s/it] \n",
      "100%|██████████| 119/119 [04:14<00:00,  2.14s/it]\n",
      "100%|██████████| 119/119 [04:25<00:00,  2.23s/it]\n",
      "100%|██████████| 119/119 [02:29<00:00,  1.25s/it]\n",
      "100%|██████████| 119/119 [04:07<00:00,  2.08s/it]\n",
      "100%|██████████| 119/119 [04:10<00:00,  2.11s/it]\n",
      "100%|██████████| 119/119 [04:00<00:00,  2.02s/it]\n",
      "100%|██████████| 119/119 [04:17<00:00,  2.16s/it]\n",
      "100%|██████████| 119/119 [03:58<00:00,  2.00s/it]\n",
      "100%|██████████| 119/119 [04:26<00:00,  2.24s/it]\n",
      "100%|██████████| 119/119 [04:46<00:00,  2.41s/it]\n",
      "100%|██████████| 119/119 [04:48<00:00,  2.43s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [08:14<00:00,  4.15s/it]\n",
      "100%|██████████| 119/119 [08:11<00:00,  4.13s/it]\n",
      "100%|██████████| 119/119 [08:03<00:00,  4.07s/it]\n",
      "100%|██████████| 119/119 [04:31<00:00,  2.28s/it]\n",
      "100%|██████████| 119/119 [08:19<00:00,  4.20s/it]\n",
      "100%|██████████| 119/119 [08:20<00:00,  4.20s/it]\n",
      "100%|██████████| 119/119 [08:06<00:00,  4.09s/it]\n",
      "100%|██████████| 119/119 [07:49<00:00,  3.94s/it]\n",
      "100%|██████████| 119/119 [08:10<00:00,  4.13s/it]\n",
      "100%|██████████| 119/119 [08:19<00:00,  4.20s/it]\n",
      "100%|██████████| 119/119 [08:38<00:00,  4.36s/it]\n",
      "100%|██████████| 119/119 [08:27<00:00,  4.27s/it]\n",
      "/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 119/119 [05:52<00:00,  2.96s/it] \n",
      "100%|██████████| 119/119 [04:09<00:00,  2.10s/it]\n",
      "100%|██████████| 119/119 [04:12<00:00,  2.12s/it]\n",
      "100%|██████████| 119/119 [02:29<00:00,  1.26s/it]\n",
      "100%|██████████| 119/119 [04:16<00:00,  2.16s/it]\n",
      "100%|██████████| 119/119 [04:15<00:00,  2.14s/it]\n",
      "100%|██████████| 119/119 [03:58<00:00,  2.00s/it]\n",
      "100%|██████████| 119/119 [03:40<00:00,  1.85s/it]\n",
      "100%|██████████| 119/119 [03:48<00:00,  1.92s/it]\n",
      "100%|██████████| 119/119 [04:07<00:00,  2.08s/it]\n",
      "100%|██████████| 119/119 [04:46<00:00,  2.41s/it]\n",
      "100%|██████████| 119/119 [04:35<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "benchmarking_folder = Path('/data/Benchmarking/')\n",
    "results_folder      = Path('/data/Results/')\n",
    "\n",
    "vmodels = ['gemma2','llama3.1']\n",
    "\n",
    "for exp in benchmarking_folder.iterdir():\n",
    "    if '.ipynb' in exp.name: continue\n",
    "    exp_name = exp.stem\n",
    "\n",
    "    for vmodel in vmodels:\n",
    "        verifier = Verifier(model = vmodel)\n",
    "        save_folder = results_folder/exp_name\n",
    "        save_folder.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "        for model in exp.iterdir():\n",
    "            if '.ipynb' in model.name: continue\n",
    "            model_name = model.name\n",
    "   \n",
    "            question_df = pd.read_csv(model / 'all_questions.csv')\n",
    "\n",
    "            save_dir = save_folder / vmodel\n",
    "            save_dir.mkdir(exist_ok=True,parents=True)\n",
    "            if (save_dir/f'{model_name}.csv').is_file():\n",
    "                print('Skipping!')\n",
    "                continue\n",
    "\n",
    "            verifier.judge_all_questions(question_df, model_name, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4cbb9d-3d86-431e-82be-15091d5e6e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
